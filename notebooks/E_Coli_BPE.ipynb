{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v4gy0lbsJYi",
        "outputId": "4aa96c22-291c-4f80-d60f-b9f2252e74ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# Configuration\n",
        "ECOLI_URL = \"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz\"\n",
        "TARGET_VOCAB_SIZE = 5000\n",
        "TARGET_COMPRESSION = 3.2\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import os\n",
        "\n",
        "def download_ecoli():\n",
        "    \"\"\"Download E. coli genome from NCBI\"\"\"\n",
        "    filename = \"ecoli_genome.fna.gz\"\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"âœ“ {filename} already exists\")\n",
        "        return filename\n",
        "\n",
        "    print(\"Downloading E. coli genome...\")\n",
        "    response = requests.get(ECOLI_URL)\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"âœ“ Downloaded {len(response.content)} bytes\")\n",
        "    return filename\n",
        "\n",
        "# Download the file\n",
        "genome_file = download_ecoli()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbEPDEvJsR3V",
        "outputId": "2cc91ced-0fbb-4b94-a0be-c3671b516609"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading E. coli genome...\n",
            "âœ“ Downloaded 1379902 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(genome_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q9tPHLMscHD",
        "outputId": "76e4c9df-5a20-4e20-bc2c-9321808d186d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ecoli_genome.fna.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fasta(filename):\n",
        "    \"\"\"\n",
        "    Load FASTA file and return DNA sequence as a single string\n",
        "\n",
        "    Args:\n",
        "        filename: path to .fna.gz file\n",
        "\n",
        "    Returns:\n",
        "        string: concatenated DNA sequence\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "\n",
        "    # Open gzip-compressed file\n",
        "    with gzip.open(filename, 'rt') as f:  # 'rt' = read text mode\n",
        "        for line in f:\n",
        "            line = line.strip()  # Remove leading/trailing whitespace\n",
        "\n",
        "            # Skip empty lines\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Skip header lines (start with '>')\n",
        "            if line.startswith('>'):\n",
        "                continue\n",
        "\n",
        "            # This is a sequence line - add it\n",
        "            sequences.append(line.upper())\n",
        "\n",
        "    # Concatenate all sequences into one string\n",
        "    corpus = ''.join(sequences)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "\n",
        "# Load the genome\n",
        "print(\"Loading E. coli genome...\")\n",
        "corpus = load_fasta(genome_file)\n",
        "\n",
        "print(f\"âœ“ Loaded {len(corpus):,} base pairs\")\n",
        "print(f\"First 100 bases: {corpus[:100]}\")\n",
        "print(f\"Character counts: {Counter(corpus)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2cgkfv5sUkg",
        "outputId": "f6426003-d08f-4f32-caf5-36ab79f8bf8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading E. coli genome...\n",
            "âœ“ Loaded 4,641,652 base pairs\n",
            "First 100 bases: AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGCTTCTGAACTGGTTACCTGCCGTGAGTAAAT\n",
            "Character counts: Counter({'C': 1180091, 'G': 1177437, 'A': 1142742, 'T': 1141382})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_vocab(corpus):\n",
        "    \"\"\"\n",
        "    Create initial vocabulary from unique characters in corpus\n",
        "\n",
        "    Args:\n",
        "        corpus: DNA sequence string\n",
        "\n",
        "    Returns:\n",
        "        dict mapping characters to IDs\n",
        "    \"\"\"\n",
        "    # Get all unique characters in the corpus\n",
        "    unique_chars = sorted(set(corpus))\n",
        "\n",
        "    # Create vocabulary dictionary: character -> ID\n",
        "    # IDs start from 0 and increment\n",
        "    vocab = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "# Initialize\n",
        "vocab = initialize_vocab(corpus)\n",
        "print(f\"Initial vocabulary: {vocab}\")\n",
        "print(f\"Initial vocab size: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ClxqVoIt0Ix",
        "outputId": "1a285701-fcdf-4cf2-c643-d7261ae4125c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial vocabulary: {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
            "Initial vocab size: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pair_counts(tokens):\n",
        "    \"\"\"\n",
        "    Count frequency of all adjacent token pairs\n",
        "\n",
        "    Args:\n",
        "        tokens: list of current tokens (strings)\n",
        "\n",
        "    Returns:\n",
        "        Counter object with {(token1, token2): count}\n",
        "    \"\"\"\n",
        "    # Create empty counter to store pair frequencies\n",
        "    pairs = []\n",
        "\n",
        "    # Iterate through tokens, creating adjacent pairs\n",
        "    # Stop at len(tokens)-1 because we need token[i] and token[i+1]\n",
        "    for i in range(len(tokens) - 1):\n",
        "        pair = (tokens[i], tokens[i + 1])\n",
        "        pairs.append(pair)\n",
        "\n",
        "    # Count how many times each pair appears\n",
        "    pair_counts = Counter(pairs)\n",
        "\n",
        "    return pair_counts\n",
        "\n",
        "\n",
        "def merge_pair(tokens, pair, new_token):\n",
        "    \"\"\"\n",
        "    Replace all occurrences of pair with new_token\n",
        "\n",
        "    Args:\n",
        "        tokens: list of current tokens\n",
        "        pair: tuple (token1, token2) to merge\n",
        "        new_token: string to replace pair with\n",
        "\n",
        "    Returns:\n",
        "        new list of tokens with pairs merged\n",
        "    \"\"\"\n",
        "    new_tokens = []\n",
        "    i = 0\n",
        "\n",
        "    # Iterate through tokens\n",
        "    while i < len(tokens):\n",
        "        # Check if we found the pair we're looking for\n",
        "        # Need to check i+1 exists and that current + next match the pair\n",
        "        if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:\n",
        "            # Found the pair! Replace with merged token\n",
        "            new_tokens.append(new_token)\n",
        "            i += 2  # Skip both tokens in the pair\n",
        "        else:\n",
        "            # Not the pair we're looking for, keep token as-is\n",
        "            new_tokens.append(tokens[i])\n",
        "            i += 1  # Move to next token\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "\n",
        "# Test function\n",
        "print(\"Testing BPE functions...\")\n",
        "test_tokens = ['A', 'T', 'C', 'G', 'A', 'T']\n",
        "print(f\"Test tokens: {test_tokens}\")\n",
        "\n",
        "test_counts = get_pair_counts(test_tokens)\n",
        "print(f\"Pair counts: {test_counts}\")\n",
        "\n",
        "test_merged = merge_pair(test_tokens, ('A', 'T'), 'AT')\n",
        "print(f\"After merging ('A', 'T') -> 'AT': {test_merged}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAGomSXluMrc",
        "outputId": "8f02aea0-3c9c-4629-b911-3a9262871228"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing BPE functions...\n",
            "Test tokens: ['A', 'T', 'C', 'G', 'A', 'T']\n",
            "Pair counts: Counter({('A', 'T'): 2, ('T', 'C'): 1, ('C', 'G'): 1, ('G', 'A'): 1})\n",
            "After merging ('A', 'T') -> 'AT': ['AT', 'C', 'G', 'AT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bpe(corpus, target_vocab_size):\n",
        "    \"\"\"\n",
        "    Train BPE tokenizer on corpus\n",
        "\n",
        "    Args:\n",
        "        corpus: DNA sequence string\n",
        "        target_vocab_size: desired vocabulary size\n",
        "\n",
        "    Returns:\n",
        "        (vocab, merge_rules, final_tokens)\n",
        "    \"\"\"\n",
        "    print(\"Starting BPE training...\")\n",
        "    print(f\"Corpus size: {len(corpus):,} bases\")\n",
        "    print(f\"Target vocab size: {target_vocab_size:,}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # STEP 1: Convert corpus to list of characters (initial tokens)\n",
        "    # Each character becomes its own token\n",
        "    tokens = list(corpus)\n",
        "    print(f\"Initial tokens: {len(tokens):,}\")\n",
        "\n",
        "    # STEP 2: Initialize vocabulary with unique characters\n",
        "    vocab = initialize_vocab(corpus)\n",
        "    initial_vocab_size = len(vocab)\n",
        "    print(f\"Initial vocab size: {initial_vocab_size}\")\n",
        "\n",
        "    # STEP 3: Calculate how many merges we need\n",
        "    num_merges = target_vocab_size - initial_vocab_size\n",
        "    print(f\"Number of merges needed: {num_merges:,}\")\n",
        "\n",
        "    # STEP 4: Store merge rules (order matters for encoding later!)\n",
        "    merge_rules = []\n",
        "\n",
        "    # STEP 5: Main training loop - perform merges until we hit target vocab size\n",
        "    print(\"\\nTraining progress:\")\n",
        "    for i in range(num_merges):\n",
        "        # Count all adjacent pairs in current token list\n",
        "        pair_counts = get_pair_counts(tokens)\n",
        "\n",
        "        # Check if we have any pairs left (shouldn't happen, but safety check)\n",
        "        if not pair_counts:\n",
        "            print(f\"\\nNo more pairs to merge at iteration {i}\")\n",
        "            break\n",
        "\n",
        "        # Find the most frequent pair (greedy approach)\n",
        "        best_pair = max(pair_counts, key=pair_counts.get)\n",
        "        best_count = pair_counts[best_pair]\n",
        "\n",
        "        # Create new token by concatenating the pair\n",
        "        # e.g., ('A', 'T') becomes 'AT'\n",
        "        new_token = best_pair[0] + best_pair[1]\n",
        "\n",
        "        # Add new token to vocabulary with next available ID\n",
        "        vocab[new_token] = len(vocab)\n",
        "\n",
        "        # Record this merge rule (needed for encoding later)\n",
        "        merge_rules.append(best_pair)\n",
        "\n",
        "        # Actually perform the merge - replace all occurrences in corpus\n",
        "        tokens = merge_pair(tokens, best_pair, new_token)\n",
        "\n",
        "        # Print progress every 100 iterations\n",
        "        if (i + 1) % 100 == 0:\n",
        "            compression = len(corpus) / len(tokens)\n",
        "            print(f\"  Merge {i+1:>4}/{num_merges}: \"\n",
        "                  f\"Merged {best_pair} ({best_count:,} occurrences) -> '{new_token}' | \"\n",
        "                  f\"Tokens: {len(tokens):,} | \"\n",
        "                  f\"Compression: {compression:.2f}x\")\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nâœ“ Training complete in {elapsed:.2f} seconds\")\n",
        "    print(f\"Final vocab size: {len(vocab):,}\")\n",
        "    print(f\"Final token count: {len(tokens):,}\")\n",
        "\n",
        "    return vocab, merge_rules, tokens\n",
        "\n",
        "\n",
        "# Train the tokenizer\n",
        "vocab, merge_rules, compressed_tokens = train_bpe(corpus, TARGET_VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jX-xnhEu0OG",
        "outputId": "6333dccd-037a-4ad8-d8b8-8be6bc678e20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting BPE training...\n",
            "Corpus size: 4,641,652 bases\n",
            "Target vocab size: 5,000\n",
            "Initial tokens: 4,641,652\n",
            "Initial vocab size: 4\n",
            "Number of merges needed: 4,996\n",
            "\n",
            "Training progress:\n",
            "  Merge  100/4996: Merged ('TT', 'TTC') (2,830 occurrences) -> 'TTTTC' | Tokens: 1,575,822 | Compression: 2.95x\n",
            "  Merge  200/4996: Merged ('GCC', 'AGC') (1,297 occurrences) -> 'GCCAGC' | Tokens: 1,387,661 | Compression: 3.34x\n",
            "  Merge  300/4996: Merged ('AT', 'GCCG') (722 occurrences) -> 'ATGCCG' | Tokens: 1,291,770 | Compression: 3.59x\n",
            "  Merge  400/4996: Merged ('T', 'AGAA') (483 occurrences) -> 'TAGAA' | Tokens: 1,232,932 | Compression: 3.76x\n",
            "  Merge  500/4996: Merged ('TAAC', 'CC') (372 occurrences) -> 'TAACCC' | Tokens: 1,190,830 | Compression: 3.90x\n",
            "  Merge  600/4996: Merged ('GC', 'TCG') (291 occurrences) -> 'GCTCG' | Tokens: 1,158,195 | Compression: 4.01x\n",
            "  Merge  700/4996: Merged ('T', 'ACCCG') (241 occurrences) -> 'TACCCG' | Tokens: 1,131,648 | Compression: 4.10x\n",
            "  Merge  800/4996: Merged ('TCTG', 'GG') (196 occurrences) -> 'TCTGGG' | Tokens: 1,110,102 | Compression: 4.18x\n",
            "  Merge  900/4996: Merged ('TTTG', 'TG') (166 occurrences) -> 'TTTGTG' | Tokens: 1,092,002 | Compression: 4.25x\n",
            "  Merge 1000/4996: Merged ('ATT', 'TTTT') (146 occurrences) -> 'ATTTTTT' | Tokens: 1,076,436 | Compression: 4.31x\n",
            "  Merge 1100/4996: Merged ('TTTGC', 'GG') (126 occurrences) -> 'TTTGCGG' | Tokens: 1,063,012 | Compression: 4.37x\n",
            "  Merge 1200/4996: Merged ('TTG', 'TGG') (110 occurrences) -> 'TTGTGG' | Tokens: 1,051,287 | Compression: 4.42x\n",
            "  Merge 1300/4996: Merged ('TCG', 'GCGC') (100 occurrences) -> 'TCGGCGC' | Tokens: 1,040,821 | Compression: 4.46x\n",
            "  Merge 1400/4996: Merged ('AGC', 'TGGC') (90 occurrences) -> 'AGCTGGC' | Tokens: 1,031,342 | Compression: 4.50x\n",
            "  Merge 1500/4996: Merged ('TC', 'TGCTG') (82 occurrences) -> 'TCTGCTG' | Tokens: 1,022,679 | Compression: 4.54x\n",
            "  Merge 1600/4996: Merged ('ACG', 'GCGG') (76 occurrences) -> 'ACGGCGG' | Tokens: 1,014,723 | Compression: 4.57x\n",
            "  Merge 1700/4996: Merged ('AGC', 'ATAA') (71 occurrences) -> 'AGCATAA' | Tokens: 1,007,382 | Compression: 4.61x\n",
            "  Merge 1800/4996: Merged ('TGAA', 'AGC') (66 occurrences) -> 'TGAAAGC' | Tokens: 1,000,556 | Compression: 4.64x\n",
            "  Merge 1900/4996: Merged ('TT', 'AGCAA') (62 occurrences) -> 'TTAGCAA' | Tokens: 994,151 | Compression: 4.67x\n",
            "  Merge 2000/4996: Merged ('TTACC', 'TG') (59 occurrences) -> 'TTACCTG' | Tokens: 988,142 | Compression: 4.70x\n",
            "  Merge 2100/4996: Merged ('ATT', 'TCAA') (55 occurrences) -> 'ATTTCAA' | Tokens: 982,456 | Compression: 4.72x\n",
            "  Merge 2200/4996: Merged ('TTTG', 'TGG') (52 occurrences) -> 'TTTGTGG' | Tokens: 977,098 | Compression: 4.75x\n",
            "  Merge 2300/4996: Merged ('ACC', 'TTTG') (49 occurrences) -> 'ACCTTTG' | Tokens: 972,050 | Compression: 4.78x\n",
            "  Merge 2400/4996: Merged ('GCCG', 'TC') (47 occurrences) -> 'GCCGTC' | Tokens: 967,252 | Compression: 4.80x\n",
            "  Merge 2500/4996: Merged ('TTTGCC', 'AG') (45 occurrences) -> 'TTTGCCAG' | Tokens: 962,675 | Compression: 4.82x\n",
            "  Merge 2600/4996: Merged ('ACG', 'ATGG') (43 occurrences) -> 'ACGATGG' | Tokens: 958,301 | Compression: 4.84x\n",
            "  Merge 2700/4996: Merged ('TCGC', 'TAA') (41 occurrences) -> 'TCGCTAA' | Tokens: 954,129 | Compression: 4.86x\n",
            "  Merge 2800/4996: Merged ('GCGC', 'TAC') (39 occurrences) -> 'GCGCTAC' | Tokens: 950,153 | Compression: 4.89x\n",
            "  Merge 2900/4996: Merged ('GGAA', 'TAA') (37 occurrences) -> 'GGAATAA' | Tokens: 946,342 | Compression: 4.90x\n",
            "  Merge 3000/4996: Merged ('TTAA', 'TGG') (36 occurrences) -> 'TTAATGG' | Tokens: 942,698 | Compression: 4.92x\n",
            "  Merge 3100/4996: Merged ('TTAA', 'AGG') (34 occurrences) -> 'TTAAAGG' | Tokens: 939,195 | Compression: 4.94x\n",
            "  Merge 3200/4996: Merged ('ATCG', 'GCGC') (33 occurrences) -> 'ATCGGCGC' | Tokens: 935,854 | Compression: 4.96x\n",
            "  Merge 3300/4996: Merged ('TAAAA', 'AAG') (32 occurrences) -> 'TAAAAAAG' | Tokens: 932,623 | Compression: 4.98x\n",
            "  Merge 3400/4996: Merged ('GCC', 'ACTG') (31 occurrences) -> 'GCCACTG' | Tokens: 929,503 | Compression: 4.99x\n",
            "  Merge 3500/4996: Merged ('ATAA', 'TCAG') (30 occurrences) -> 'ATAATCAG' | Tokens: 926,485 | Compression: 5.01x\n",
            "  Merge 3600/4996: Merged ('TTGC', 'TGCG') (28 occurrences) -> 'TTGCTGCG' | Tokens: 923,584 | Compression: 5.03x\n",
            "  Merge 3700/4996: Merged ('GCGC', 'AACC') (27 occurrences) -> 'GCGCAACC' | Tokens: 920,791 | Compression: 5.04x\n",
            "  Merge 3800/4996: Merged ('AGCAG', 'TTC') (27 occurrences) -> 'AGCAGTTC' | Tokens: 918,091 | Compression: 5.06x\n",
            "  Merge 3900/4996: Merged ('GCGC', 'AGTG') (26 occurrences) -> 'GCGCAGTG' | Tokens: 915,491 | Compression: 5.07x\n",
            "  Merge 4000/4996: Merged ('TT', 'ATTCAG') (25 occurrences) -> 'TTATTCAG' | Tokens: 912,970 | Compression: 5.08x\n",
            "  Merge 4100/4996: Merged ('AGCC', 'CCC') (24 occurrences) -> 'AGCCCCC' | Tokens: 910,506 | Compression: 5.10x\n",
            "  Merge 4200/4996: Merged ('TT', 'ATAAAA') (24 occurrences) -> 'TTATAAAA' | Tokens: 908,106 | Compression: 5.11x\n",
            "  Merge 4300/4996: Merged ('TTGG', 'TGC') (23 occurrences) -> 'TTGGTGC' | Tokens: 905,799 | Compression: 5.12x\n",
            "  Merge 4400/4996: Merged ('TAGCG', 'AG') (22 occurrences) -> 'TAGCGAG' | Tokens: 903,549 | Compression: 5.14x\n",
            "  Merge 4500/4996: Merged ('ATATT', 'CCC') (22 occurrences) -> 'ATATTCCC' | Tokens: 901,349 | Compression: 5.15x\n",
            "  Merge 4600/4996: Merged ('TATC', 'GCCG') (21 occurrences) -> 'TATCGCCG' | Tokens: 899,236 | Compression: 5.16x\n",
            "  Merge 4700/4996: Merged ('AC', 'ACCAA') (21 occurrences) -> 'ACACCAA' | Tokens: 897,136 | Compression: 5.17x\n",
            "  Merge 4800/4996: Merged ('TGG', 'ATCG') (20 occurrences) -> 'TGGATCG' | Tokens: 895,134 | Compression: 5.19x\n",
            "  Merge 4900/4996: Merged ('TCAC', 'AAC') (19 occurrences) -> 'TCACAAC' | Tokens: 893,140 | Compression: 5.20x\n",
            "\n",
            "âœ“ Training complete in 5300.68 seconds\n",
            "Final vocab size: 5,000\n",
            "Final token count: 891,316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_compression_ratio(original_corpus, compressed_tokens):\n",
        "    \"\"\"\n",
        "    Calculate compression ratio\n",
        "\n",
        "    Args:\n",
        "        original_corpus: original DNA string\n",
        "        compressed_tokens: list of tokens after BPE\n",
        "\n",
        "    Returns:\n",
        "        float: compression ratio\n",
        "    \"\"\"\n",
        "    # Original size = number of bytes (characters in string)\n",
        "    original_size = len(original_corpus)\n",
        "\n",
        "    # Compressed size = number of tokens after merging\n",
        "    compressed_size = len(compressed_tokens)\n",
        "\n",
        "    # Compression ratio = how many bytes each token represents on average\n",
        "    compression_ratio = original_size / compressed_size\n",
        "\n",
        "    return compression_ratio\n",
        "\n",
        "\n",
        "# Calculate metrics\n",
        "compression_ratio = calculate_compression_ratio(corpus, compressed_tokens)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Vocabulary size: {len(vocab):,} tokens\")\n",
        "print(f\"Original size: {len(corpus):,} bytes (characters)\")\n",
        "print(f\"Compressed size: {len(compressed_tokens):,} tokens\")\n",
        "print(f\"Compression ratio: {compression_ratio:.3f}x\")\n",
        "print(f\"Average token length: {len(corpus) / len(compressed_tokens):.3f} bases\")\n",
        "\n",
        "# Additional statistics\n",
        "avg_token_length = sum(len(token) for token in vocab.keys()) / len(vocab)\n",
        "max_token_length = max(len(token) for token in vocab.keys())\n",
        "print(f\"Average token length (vocab): {avg_token_length:.2f} bases\")\n",
        "print(f\"Longest token: {max_token_length} bases\")\n",
        "\n",
        "# Check requirements\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"REQUIREMENTS CHECK\")\n",
        "print(\"=\"*60)\n",
        "vocab_check = \"âœ“ PASS\" if len(vocab) >= 5000 else \"âœ— FAIL\"\n",
        "compression_check = \"âœ“ PASS\" if compression_ratio >= 3.2 else \"âœ— FAIL\"\n",
        "\n",
        "print(f\"{vocab_check} Vocabulary size â‰¥ 5000: {len(vocab):,}\")\n",
        "print(f\"{compression_check} Compression ratio â‰¥ 3.2: {compression_ratio:.3f}\")\n",
        "\n",
        "if len(vocab) >= 5000 and compression_ratio >= 3.2:\n",
        "    print(\"\\nðŸŽ‰ ALL REQUIREMENTS MET! ðŸŽ‰\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Some requirements not met. Consider training longer.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZseCm8PbFc0I",
        "outputId": "1c78dc5c-4f3a-4ae4-cffc-714933993bdf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL RESULTS\n",
            "============================================================\n",
            "Vocabulary size: 5,000 tokens\n",
            "Original size: 4,641,652 bytes (characters)\n",
            "Compressed size: 891,316 tokens\n",
            "Compression ratio: 5.208x\n",
            "Average token length: 5.208 bases\n",
            "Average token length (vocab): 6.91 bases\n",
            "Longest token: 26 bases\n",
            "\n",
            "============================================================\n",
            "REQUIREMENTS CHECK\n",
            "============================================================\n",
            "âœ“ PASS Vocabulary size â‰¥ 5000: 5,000\n",
            "âœ“ PASS Compression ratio â‰¥ 3.2: 5.208\n",
            "\n",
            "ðŸŽ‰ ALL REQUIREMENTS MET! ðŸŽ‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_vocabulary(vocab, merge_rules):\n",
        "    \"\"\"\n",
        "    Analyze what patterns BPE learned\n",
        "\n",
        "    Args:\n",
        "        vocab: trained vocabulary dict\n",
        "        merge_rules: list of merge operations\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VOCABULARY ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # SECTION 1: Token length distribution\n",
        "    print(\"\\n1. TOKEN LENGTH DISTRIBUTION\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    token_lengths = {}\n",
        "    for token in vocab.keys():\n",
        "        length = len(token)\n",
        "        token_lengths[length] = token_lengths.get(length, 0) + 1\n",
        "\n",
        "    # Sort by length and display\n",
        "    for length in sorted(token_lengths.keys()):\n",
        "        count = token_lengths[length]\n",
        "        bar = \"â–ˆ\" * (count // 50)  # Visual bar chart\n",
        "        print(f\"  Length {length:2d}: {count:4d} tokens {bar}\")\n",
        "\n",
        "\n",
        "    # SECTION 2: Biologically meaningful patterns\n",
        "    print(\"\\n2. BIOLOGICAL PATTERNS FOUND\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Define known biological motifs\n",
        "    biological_patterns = {\n",
        "        'Start Codons': ['ATG'],\n",
        "        'Stop Codons': ['TAA', 'TAG', 'TGA'],\n",
        "        'TATA Box': ['TATA', 'TATAA', 'TATAAA', 'TATATAT'],\n",
        "        'Shine-Dalgarno (ribosome binding)': ['AGGAGG', 'AGGA', 'GGAGG'],\n",
        "        'Poly-A tail': ['AAA', 'AAAA', 'AAAAA', 'AAAAAA'],\n",
        "        'Poly-T': ['TTT', 'TTTT', 'TTTTT'],\n",
        "        'CpG dinucleotides': ['CG', 'CGCG', 'GCGC'],\n",
        "        'Homopolymer runs': ['GGG', 'GGGG', 'CCC', 'CCCC'],\n",
        "    }\n",
        "\n",
        "    found_any = False\n",
        "    for category, patterns in biological_patterns.items():\n",
        "        found_patterns = [p for p in patterns if p in vocab]\n",
        "        if found_patterns:\n",
        "            found_any = True\n",
        "            print(f\"\\n  {category}:\")\n",
        "            for pattern in found_patterns:\n",
        "                print(f\"    âœ“ {pattern} (ID: {vocab[pattern]})\")\n",
        "\n",
        "    if not found_any:\n",
        "        print(\"  No well-known biological patterns found in vocabulary\")\n",
        "        print(\"  (This is normal - BPE learns statistical patterns, not biology)\")\n",
        "\n",
        "\n",
        "    # SECTION 3: Most common initial merges\n",
        "    print(\"\\n3. FIRST 20 MERGES (Most Common Patterns)\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, pair in enumerate(merge_rules[:20]):\n",
        "        merged_token = pair[0] + pair[1]\n",
        "        print(f\"  Merge {i+1:2d}: {pair[0]:8s} + {pair[1]:8s} = {merged_token}\")\n",
        "\n",
        "\n",
        "    # SECTION 4: Longest tokens learned\n",
        "    print(\"\\n4. LONGEST TOKENS (Top 20)\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Sort all tokens by length\n",
        "    sorted_by_length = sorted(vocab.keys(), key=len, reverse=True)\n",
        "\n",
        "    for i, token in enumerate(sorted_by_length[:20]):\n",
        "        print(f\"  {i+1:2d}. {token:20s} (length: {len(token)}, ID: {vocab[token]})\")\n",
        "\n",
        "\n",
        "    # SECTION 5: Sample random tokens from middle of vocabulary\n",
        "    print(\"\\n5. SAMPLE TOKENS FROM MIDDLE OF TRAINING\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Get tokens created around merge 2500 (middle of training)\n",
        "    if len(merge_rules) >= 2500:\n",
        "        middle_merges = merge_rules[2400:2420]\n",
        "        print(\"  Tokens created around merge #2500:\")\n",
        "        for pair in middle_merges:\n",
        "            merged = pair[0] + pair[1]\n",
        "            print(f\"    {merged}\")\n",
        "\n",
        "\n",
        "    # SECTION 6: Character frequency in vocabulary\n",
        "    print(\"\\n6. CHARACTER FREQUENCY IN VOCABULARY TOKENS\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    char_counts = Counter()\n",
        "    for token in vocab.keys():\n",
        "        char_counts.update(token)\n",
        "\n",
        "    total_chars = sum(char_counts.values())\n",
        "    for char in sorted(char_counts.keys()):\n",
        "        count = char_counts[char]\n",
        "        percentage = (count / total_chars) * 100\n",
        "        bar = \"â–ˆ\" * int(percentage)\n",
        "        print(f\"  {char}: {count:6d} ({percentage:5.2f}%) {bar}\")\n",
        "\n",
        "\n",
        "# Run analysis\n",
        "analyze_vocabulary(vocab, merge_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq8siougFtEu",
        "outputId": "b42b2d72-c766-41e0-d847-a6c081affdf0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "VOCABULARY ANALYSIS\n",
            "============================================================\n",
            "\n",
            "1. TOKEN LENGTH DISTRIBUTION\n",
            "------------------------------------------------------------\n",
            "  Length  1:    4 tokens \n",
            "  Length  2:   10 tokens \n",
            "  Length  3:   25 tokens \n",
            "  Length  4:   89 tokens â–ˆ\n",
            "  Length  5:  300 tokens â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  Length  6:  971 tokens â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  Length  7: 2284 tokens â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  Length  8: 1184 tokens â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  Length  9:  109 tokens â–ˆâ–ˆ\n",
            "  Length 10:    9 tokens \n",
            "  Length 11:    4 tokens \n",
            "  Length 12:    2 tokens \n",
            "  Length 13:    3 tokens \n",
            "  Length 16:    3 tokens \n",
            "  Length 18:    2 tokens \n",
            "  Length 26:    1 tokens \n",
            "\n",
            "2. BIOLOGICAL PATTERNS FOUND\n",
            "------------------------------------------------------------\n",
            "\n",
            "  Start Codons:\n",
            "    âœ“ ATG (ID: 20)\n",
            "\n",
            "  Stop Codons:\n",
            "    âœ“ TAA (ID: 25)\n",
            "    âœ“ TAG (ID: 65)\n",
            "\n",
            "  TATA Box:\n",
            "    âœ“ TATAA (ID: 279)\n",
            "\n",
            "  Shine-Dalgarno (ribosome binding):\n",
            "    âœ“ AGGAGG (ID: 3642)\n",
            "\n",
            "  Poly-A tail:\n",
            "    âœ“ AAAA (ID: 34)\n",
            "    âœ“ AAAAAA (ID: 219)\n",
            "\n",
            "  Poly-T:\n",
            "    âœ“ TTT (ID: 55)\n",
            "    âœ“ TTTT (ID: 40)\n",
            "\n",
            "  CpG dinucleotides:\n",
            "    âœ“ GCGC (ID: 35)\n",
            "\n",
            "  Homopolymer runs:\n",
            "    âœ“ GGG (ID: 38)\n",
            "    âœ“ GGGG (ID: 342)\n",
            "    âœ“ CCC (ID: 102)\n",
            "\n",
            "3. FIRST 20 MERGES (Most Common Patterns)\n",
            "------------------------------------------------------------\n",
            "  Merge  1: G        + C        = GC\n",
            "  Merge  2: T        + T        = TT\n",
            "  Merge  3: A        + A        = AA\n",
            "  Merge  4: T        + C        = TC\n",
            "  Merge  5: A        + C        = AC\n",
            "  Merge  6: T        + G        = TG\n",
            "  Merge  7: G        + G        = GG\n",
            "  Merge  8: A        + G        = AG\n",
            "  Merge  9: GC       + C        = GCC\n",
            "  Merge 10: A        + TC       = ATC\n",
            "  Merge 11: A        + TT       = ATT\n",
            "  Merge 12: A        + T        = AT\n",
            "  Merge 13: AA       + C        = AAC\n",
            "  Merge 14: GC       + G        = GCG\n",
            "  Merge 15: AC       + C        = ACC\n",
            "  Merge 16: TT       + C        = TTC\n",
            "  Merge 17: A        + TG       = ATG\n",
            "  Merge 18: A        + GC       = AGC\n",
            "  Merge 19: T        + GC       = TGC\n",
            "  Merge 20: TG       + G        = TGG\n",
            "\n",
            "4. LONGEST TOKENS (Top 20)\n",
            "------------------------------------------------------------\n",
            "   1. ATGCGGCGTGAACGCCTTATCCGGCC (length: 26, ID: 4480)\n",
            "   2. GCGTTCACGCCGCATCCG   (length: 18, ID: 2569)\n",
            "   3. GCGTTTACGCCGCATCCG   (length: 18, ID: 3433)\n",
            "   4. ATGCGGCGTGAACGCC     (length: 16, ID: 2769)\n",
            "   5. ATGCGGCGTAAACGCC     (length: 16, ID: 4053)\n",
            "   6. ATGCGACGCTGGCGCG     (length: 16, ID: 4218)\n",
            "   7. TAGGCCGGATAAG        (length: 13, ID: 2341)\n",
            "   8. TAGGTCGGATAAG        (length: 13, ID: 4115)\n",
            "   9. ATGCGCTACGCTT        (length: 13, ID: 4292)\n",
            "  10. ACGCCGCATCCG         (length: 12, ID: 2220)\n",
            "  11. TCTTATCAGGCC         (length: 12, ID: 3604)\n",
            "  12. ATGCGACGCTG          (length: 11, ID: 2373)\n",
            "  13. ATGCGGCGTAA          (length: 11, ID: 3441)\n",
            "  14. ATAAGCGTAGC          (length: 11, ID: 3913)\n",
            "  15. TCGCATCAGGC          (length: 11, ID: 4383)\n",
            "  16. GCCGGATAAG           (length: 10, ID: 1607)\n",
            "  17. TTATCCGGCC           (length: 10, ID: 1711)\n",
            "  18. ATGCGGCGTG           (length: 10, ID: 2251)\n",
            "  19. GCCGCATCCG           (length: 10, ID: 2476)\n",
            "  20. TTATCAGGCC           (length: 10, ID: 3456)\n",
            "\n",
            "5. SAMPLE TOKENS FROM MIDDLE OF TRAINING\n",
            "------------------------------------------------------------\n",
            "  Tokens created around merge #2500:\n",
            "    TGCGTCG\n",
            "    ATTTATG\n",
            "    TTGCTCC\n",
            "    AACGTTTT\n",
            "    ATCCCAA\n",
            "    ACTGGCAG\n",
            "    TCAGCCC\n",
            "    TGGCGTG\n",
            "    TATTTCC\n",
            "    ATTCATG\n",
            "    ACCGGCAA\n",
            "    ACGCCCG\n",
            "    GCTCGG\n",
            "    GCAACAG\n",
            "    TCCTTC\n",
            "    TGCTTAA\n",
            "    TGCCCCC\n",
            "    TCATTGG\n",
            "    AACAACC\n",
            "    ACGGGCG\n",
            "\n",
            "6. CHARACTER FREQUENCY IN VOCABULARY TOKENS\n",
            "------------------------------------------------------------\n",
            "  A:   8515 (24.66%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  C:   8688 (25.16%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  G:   8891 (25.75%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  T:   8435 (24.43%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, vocab, merge_rules):\n",
        "    \"\"\"\n",
        "    Encode new text using trained tokenizer\n",
        "\n",
        "    Args:\n",
        "        text: DNA sequence string to encode\n",
        "        vocab: trained vocabulary\n",
        "        merge_rules: list of merge rules from training\n",
        "\n",
        "    Returns:\n",
        "        list of token IDs\n",
        "    \"\"\"\n",
        "    # STEP 1: Start with character-level tokenization\n",
        "    tokens = list(text)\n",
        "\n",
        "    # STEP 2: Apply all merge rules in the SAME ORDER as training\n",
        "    # This is critical! Order matters in BPE\n",
        "    for pair in merge_rules:\n",
        "        # Create the merged token string\n",
        "        merged_token = pair[0] + pair[1]\n",
        "\n",
        "        # Replace all occurrences of this pair\n",
        "        tokens = merge_pair(tokens, pair, merged_token)\n",
        "\n",
        "    # STEP 3: Convert tokens to IDs using vocabulary\n",
        "    token_ids = []\n",
        "    for token in tokens:\n",
        "        if token in vocab:\n",
        "            token_ids.append(vocab[token])\n",
        "        else:\n",
        "            # This shouldn't happen if encoding same distribution as training\n",
        "            print(f\"Warning: Unknown token '{token}' - skipping\")\n",
        "\n",
        "    return token_ids\n",
        "\n",
        "\n",
        "def decode(token_ids, vocab):\n",
        "    \"\"\"\n",
        "    Decode token IDs back to original text\n",
        "\n",
        "    Args:\n",
        "        token_ids: list of integer token IDs\n",
        "        vocab: trained vocabulary\n",
        "\n",
        "    Returns:\n",
        "        string: decoded DNA sequence\n",
        "    \"\"\"\n",
        "    # STEP 1: Create reverse vocabulary (ID -> token string)\n",
        "    # This maps numbers back to the actual token strings\n",
        "    id_to_token = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "    # STEP 2: Look up each ID and get its token string\n",
        "    tokens = []\n",
        "    for token_id in token_ids:\n",
        "        if token_id in id_to_token:\n",
        "            tokens.append(id_to_token[token_id])\n",
        "        else:\n",
        "            print(f\"Warning: Unknown token ID {token_id}\")\n",
        "\n",
        "    # STEP 3: Concatenate all token strings to reconstruct original\n",
        "    decoded_text = ''.join(tokens)\n",
        "\n",
        "    return decoded_text\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TEST ENCODE/DECODE\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENCODE/DECODE TESTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test 1: Small sequence\n",
        "print(\"\\nTest 1: Small sequence\")\n",
        "print(\"-\" * 60)\n",
        "test_seq = \"ATCGATCGATCGATCG\"\n",
        "print(f\"Original: {test_seq}\")\n",
        "\n",
        "encoded = encode(test_seq, vocab, merge_rules)\n",
        "print(f\"Encoded:  {encoded}\")\n",
        "print(f\"Length:   {len(test_seq)} chars -> {len(encoded)} tokens\")\n",
        "print(f\"Compression: {len(test_seq) / len(encoded):.2f}x\")\n",
        "\n",
        "decoded = decode(encoded, vocab)\n",
        "print(f\"Decoded:  {decoded}\")\n",
        "print(f\"Lossless: {test_seq == decoded} âœ“\" if test_seq == decoded else f\"Lossless: {test_seq == decoded} âœ—\")\n",
        "\n",
        "\n",
        "# Test 2: Longer sequence from corpus\n",
        "print(\"\\nTest 2: Real genome sequence (1000 bases)\")\n",
        "print(\"-\" * 60)\n",
        "test_sequence = corpus[:1000]\n",
        "\n",
        "encoded = encode(test_sequence, vocab, merge_rules)\n",
        "decoded = decode(encoded, vocab)\n",
        "\n",
        "print(f\"Original length: {len(test_sequence):,} bases\")\n",
        "print(f\"Encoded length:  {len(encoded):,} tokens\")\n",
        "print(f\"Compression:     {len(test_sequence)/len(encoded):.3f}x\")\n",
        "print(f\"Lossless check:  {test_sequence == decoded} âœ“\" if test_sequence == decoded else f\"Lossless: {test_sequence == decoded} âœ—\")\n",
        "print(f\"\\nFirst 10 tokens: {encoded[:10]}\")\n",
        "print(f\"First 10 decoded strings: {[decode([tid], vocab) for tid in encoded[:10]]}\")\n",
        "\n",
        "\n",
        "# Test 3: Entire corpus (verify overall compression)\n",
        "print(\"\\nTest 3: Entire E. coli genome\")\n",
        "print(\"-\" * 60)\n",
        "full_encoded = encode(corpus, vocab, merge_rules)\n",
        "print(f\"Full genome tokens: {len(full_encoded):,}\")\n",
        "print(f\"Compression ratio:  {len(corpus) / len(full_encoded):.3f}x\")\n",
        "\n",
        "# Verify it matches our training compression\n",
        "print(f\"Training tokens:    {len(compressed_tokens):,}\")\n",
        "print(f\"Match:              {len(full_encoded) == len(compressed_tokens)} âœ“\" if len(full_encoded) == len(compressed_tokens) else f\"Match: {len(full_encoded) == len(compressed_tokens)} âš ï¸\")\n",
        "\n",
        "\n",
        "# Test 4: Show some example encodings\n",
        "print(\"\\nTest 4: Example token encodings\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "examples = [\n",
        "    \"ATG\",      # Start codon\n",
        "    \"TAA\",      # Stop codon\n",
        "    \"AAAA\",     # Poly-A\n",
        "    \"GCGCGC\",   # Repeating pattern\n",
        "]\n",
        "\n",
        "for seq in examples:\n",
        "    if seq in corpus[:100000]:  # Check if it exists in genome\n",
        "        enc = encode(seq, vocab, merge_rules)\n",
        "        dec = decode(enc, vocab)\n",
        "        tokens_str = [decode([tid], vocab) for tid in enc]\n",
        "        print(f\"{seq:12s} -> {str(enc):20s} -> {tokens_str} -> {dec}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHn0Ve1oGGQc",
        "outputId": "f83aed22-d387-47e9-a328-8a91e42bb191"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENCODE/DECODE TESTING\n",
            "============================================================\n",
            "\n",
            "Test 1: Small sequence\n",
            "------------------------------------------------------------\n",
            "Original: ATCGATCGATCGATCG\n",
            "Encoded:  [44, 44, 44, 44]\n",
            "Length:   16 chars -> 4 tokens\n",
            "Compression: 4.00x\n",
            "Decoded:  ATCGATCGATCGATCG\n",
            "Lossless: True âœ“\n",
            "\n",
            "Test 2: Real genome sequence (1000 bases)\n",
            "------------------------------------------------------------\n",
            "Original length: 1,000 bases\n",
            "Encoded length:  198 tokens\n",
            "Compression:     5.051x\n",
            "Lossless check:  True âœ“\n",
            "\n",
            "First 10 tokens: [21, 103, 43, 143, 22, 160, 3697, 1265, 9, 555]\n",
            "First 10 decoded strings: ['AGC', 'TTTTC', 'ATTC', 'TGAC', 'TGC', 'AACGG', 'GCAATATG', 'TCTCTG', 'TG', 'TGGATT']\n",
            "\n",
            "Test 3: Entire E. coli genome\n",
            "------------------------------------------------------------\n",
            "Full genome tokens: 891,316\n",
            "Compression ratio:  5.208x\n",
            "Training tokens:    891,316\n",
            "Match:              True âœ“\n",
            "\n",
            "Test 4: Example token encodings\n",
            "------------------------------------------------------------\n",
            "ATG          -> [20]                 -> ['ATG'] -> ATG\n",
            "TAA          -> [25]                 -> ['TAA'] -> TAA\n",
            "AAAA         -> [34]                 -> ['AAAA'] -> AAAA\n",
            "GCGCGC       -> [2813]               -> ['GCGCGC'] -> GCGCGC\n"
          ]
        }
      ]
    }
  ]
}